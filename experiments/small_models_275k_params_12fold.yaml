# Experiment: Small Models (~275k Parameters) - 12-Fold Cross-Validation
# Training EALSTM and Mamba models with ~275k parameters each
# Dataset: CAMELS basins with area < 2000 kmÂ² (617 basins total)
# Input window: 270 days
# Mode: Simulation (rainfall-runoff)
# Cross-validation: 12 folds (~565 train, ~51 test per fold)

models:
  # EALSTM models (275,716 parameters)
  ealstm_275k_fold_1: configs/models/ealstm_275k_params_fold_1.yaml
  ealstm_275k_fold_2: configs/models/ealstm_275k_params_fold_2.yaml
  ealstm_275k_fold_3: configs/models/ealstm_275k_params_fold_3.yaml
  ealstm_275k_fold_4: configs/models/ealstm_275k_params_fold_4.yaml
  ealstm_275k_fold_5: configs/models/ealstm_275k_params_fold_5.yaml
  ealstm_275k_fold_6: configs/models/ealstm_275k_params_fold_6.yaml
  ealstm_275k_fold_7: configs/models/ealstm_275k_params_fold_7.yaml
  ealstm_275k_fold_8: configs/models/ealstm_275k_params_fold_8.yaml
  ealstm_275k_fold_9: configs/models/ealstm_275k_params_fold_9.yaml
  ealstm_275k_fold_10: configs/models/ealstm_275k_params_fold_10.yaml
  ealstm_275k_fold_11: configs/models/ealstm_275k_params_fold_11.yaml
  ealstm_275k_fold_12: configs/models/ealstm_275k_params_fold_12.yaml

  # Mamba models (275,823 parameters)
  mamba_275k_fold_1: configs/models/mamba_275k_params_fold_1.yaml
  mamba_275k_fold_2: configs/models/mamba_275k_params_fold_2.yaml
  mamba_275k_fold_3: configs/models/mamba_275k_params_fold_3.yaml
  mamba_275k_fold_4: configs/models/mamba_275k_params_fold_4.yaml
  mamba_275k_fold_5: configs/models/mamba_275k_params_fold_5.yaml
  mamba_275k_fold_6: configs/models/mamba_275k_params_fold_6.yaml
  mamba_275k_fold_7: configs/models/mamba_275k_params_fold_7.yaml
  mamba_275k_fold_8: configs/models/mamba_275k_params_fold_8.yaml
  mamba_275k_fold_9: configs/models/mamba_275k_params_fold_9.yaml
  mamba_275k_fold_10: configs/models/mamba_275k_params_fold_10.yaml
  mamba_275k_fold_11: configs/models/mamba_275k_params_fold_11.yaml
  mamba_275k_fold_12: configs/models/mamba_275k_params_fold_12.yaml

trainer:
  mode: evaluation # Use train/val/test splits for model evaluation
  max_epochs: 150
  early_stopping_patience: 50 # Train for at least 50 epochs
  accumulate_grad_batches: 1 # Effective batch size: 4096 * 1 = 4096
  check_val_every_n_epoch: 1
  gradient_clip_val: 1.0
